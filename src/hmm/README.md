<h1>Hidden Markov Models</h1>

HMM is directed probabilistic graphical model.


# Components

> $O$: Oberserved sequence, ${o_1, o_2, ..., o_\tau}, o_i=v_k$.

> $H$: Hidden states, ${h_0, h_1, ..., h_\tau}, h_i=s_k$.

# Parameters

> $\pi$: Initial probability of each  hidden state.$(S)$, $S$ is the number of states.

> $T$: Transmit probability between hidden states.$(S, S)$.

> $E$: Emition probability from hidden state to output item. $(S, V)$, $V$ is the number of output vocabs.

# Problems

## 1. Probability of output sequence

> Given the model $\theta=(\pi, T, E)$, compute the probability of an output seqence $O$, $P(O | H, \theta)$.

## 2. Parameter estimation  

> Given training data of $<I, O>$, estimate the parameters $\theta=(\pi, T, E)$

## 3. Decode the hidden states

> Given the model $\theta=(\pi, T, E)$, and the output sequence $O$, get the hidden states $H$ with maximum probability, $H = argmax_{H} P(O | H, \theta)$

# Assumptions

## Markov Assumption

> $P(h_{t+1}|h_t, ..., h_1;o_t; ..., o_1; \theta) = P(h_{t+1}|h_t;\theta)$

## Time independency

> $P(h_{t+1}=j|h_t=i;\theta)=P(h_{k+1}=j|h_k=i;\theta)$

## Output independency

> $P(o_t|h_t, ..., h_1;o_{t-1}, ..., o_1;\theta)=p(o_t|h_t;\theta)$

# Problem 1: (Evaluation) Probability of output sequence

## Brute force

For an output sequence $O$, compute $P(O | \theta)$.

According to the generative principle of this probalistic graphical model, to compute $P(O|\theta)$, $H$ must be specified ($O$ is generated by $H$).

> $P(O|\theta)=\sum_{H}P(O, H|\theta)$
> $=\sum_{I}P(O|H;\theta)P(H|\theta)$

For fixed hidden states $(h_1, h_2, ..., h_\tau)$,
> $P(H|\theta)=\pi_{h_1}T(h_1,h_2)T(h_2, h_3)...T(h_{\tau-1}, h_\tau)$

For the output seqence $o_1, o_2, ..., o_T$,
> $P(O | H; \theta)=E(h_1, o_1), E(h_2, o_2)...E(h_\tau, o_\tau)$


Thus, 

> $P(O|\theta)=\sum_{h_1,h_2,...,h_\tau}P(O|H;\theta)P(H|\theta)$

> $=\sum_{h_1, h_2, ..., h_\tau}\pi_{h_1}E(h_1,o_1)T(h_1, h_2)E(h_2, o_2)...T(h_{\tau-1}, h_\tau)E(h_\tau, o_\tau)$


The time complexity is $O(\tau S^{\tau})$, $S^{\tau}$ for enumurate over the $\tau$ hidden states, $\tau$ for compute $P(O|I;\theta)$ with a fixed hidden state sequence.

## Forward Algorithm

> $P(o_1, o_2, ..., o_t | \theta) = \sum_{s=1}^S P(o_1, o_2, ..., o_t, h_t=s|\theta)$

Denote  
> $\alpha(t, s) = P(o_1, o_2, ..., o_t, h_t=s|\theta)$

Initializtion, 
> $\alpha(1, s) = P(o_1, h_1=s|\theta)=\pi_sE(s, o_1), s=1, 2, ..., S$

Iteration, 
> $\alpha(t+1, s) = P(o_1, o_2, ..., o_{t+1}, h_{t+1}=s|\theta)$

> $=\sum_{i=1}^S P(o_1, o_2, ..., o_t, h_t=i,o_{t+1}, h_{t+1}=s|\theta)$

> $=\sum_i P(o_1, o_2, ..., o_t|h_t=i, o_{t+1}, h_{t+1}=s;\theta)$

> $* P(o_{t+1}, h_t=i, h_{t+1}=s|\theta)$

> $=\sum_i P(o_1, o_2, ..., o_t|h_t=i;\theta)$ (Output independency assumption)

> $*P(o_{t+1}|h_{t+1}=s, h_t=i;\theta)$

> $* P(h_{t+1}=s, h_t=i|\theta)$

> $=\sum_i P(o_1, o_2, ..., o_t|h_t=i;\theta)$

> $*P(o_{t+1}|h_{t+1}=s;\theta)$

> $*P(h_{t+1}=s|h_t=i;\theta)*P(h_t=i|\theta)$

> $=\sum_i \{P(o_1, o_2, ..., o_t|h_t=i;\theta) * P(h_t=i|\theta)\} * E(s, o_{t+1}) * T(i, s)$

> $=\sum_i P(o_1, o_2, ..., o_t, h_t=i|\theta) * T(i, s) * E(s, o_{t+1})$

Thus, 
> $\alpha(t+1, s)=\sum_i \{\alpha(t, i) T(i, s) \} E(s, o_{t+1})$


Termination, 
> $P(O|\theta)=\sum_{s=1}^S \alpha(T, s)$

# Problem 2: (Learning) Parameter estimation 

# Problem 3: (Decoding) Decode the hidden states 



To estimate the parameters of HMM: [init_p, trans_p, emit_p]


States = ['B', 'M', 'E', 'S']
B: Begin of a word.
M: Middle of a word.
E: End of a word.
S: Single characteristic word.


Observed values: V = [vocab_0, vocab_1, ..., vocab_{N-1}], N = n_vocab
Observed variables: O, Output sequences, o_t = v[o_t], t=0,1,..., T-1
Hidden variables: H, The hidden sequences, which are the flag of segmentation,
    h_t, t=0,1, ..., T-1
Complete data: (O, H),
O = o_0, o_1, ..., o_{T-1}
H = h_0, h_1, ..., h_{T-1}

HMM assumptions:
(1) Markov assumption, limited history, one-order Markov Chain,
    P(h_t | h_{t-1}, o_{t-1}, h_{t_2}, o_{t-2}..., h_0, o_0) = P(h_t | h_{t-1})
(1) state is independent of time :  P(h_{t+1}=j | h_t=i) = P(h_{t'+1}=j | h_t'=i);
(2) output only depends on current hidden state:
    P(o_t | h_{T-1}, o_{T-1}, ..., h_0, o_0} = P(o_t | h_t)


Likelihood function of the Data:
P(D | \theta) = P(O | \theta) = \sum_h P(O, H | \theta)
    = \sum_h P(O | H; theta) * P(H | \theta)

Using Expectation-Maximization method,
the Q-function is,
Q(\theta, \theta^{old}) = E_H [ log P(O, H | \theta) | O, \theta^{old}]
= \sum_h log P(O, H | \theta) * P(H | O, \theta^{old})

P(O, H | \theta) =
    pi_{h_0} * emit_p(h_0, o_0)
    * trans_p(h_0, h_1) * emit_p(h_1, o_1)
    * trans_p(h_1, h_2) * emit_p(h_2, o_2)
    * ...
    * trans_p(h_{T-2}, h_{T-1}) * emit_p(h_{T-1}, o_{T-1})

Thus,
Q(\theta, \theta^{old}) = \sum_h {
     log pi_{h_0} * P(H | O, \theta^{old})                               # Only affects pi
    + \sum_{i=0}^{T-2} log trans_p(h_i, h_{i+1}) * P(H | O, \theta^{old})# Only affects trans_p
    + \sum_{i=0}^{T-1} log emit_p(h_i, o_i) * P(H | O, \theta^{old})     # Only affects emit_p
}

# === pi
Considering components Q_pi only affects pi in Q, s.t. \sum_s pi_s = 1
Q_pi = \sum_h log pi_{h_0} * P(H | \theta^{old})
= \sum_{s=0}^{S-1} log pi_s * P(h_0 = s | O, \theta^{old} )

Let gamma_t(s) = P(h_t = s | O, \theta)
pi_s = gamma_0(s)

Currently, I don't know why the forward and backward prob is defined, as below.

Forward probability, given hmm model(\theta), in time t, the hidden state is s,
the observed sequence from time 0 to time t is o_0, o_1, ..., o_t,
alpha_t(s) = P(o_0, o_1, ..., o_t, h_t = s | \theta)
alpha_0(s) = pi_s * trans_p(s, o_0)
alpha_t(s) = {\sum_{i=0}^{S-1} alpha_{t-1}(i) * trans(i, s)} * emit_p(s, o_t)
P(O | \theta) = \sum_{s=0}^{S-1} alpha_{T-1}(s)


Backward probability, given hmm model, in time t, the hidden state is s,
the observed sequence from time t+1 to time T-1 is o_{t+1}, o_{t+2}, ..., o_{T-1}
beta_t(s) = P(o_{t+1}, o_{t+2}, ..., o_{T-1} | h_t = s, \theta)
beta_{t-1}(s) = P(o_t, o_{t+1}, o_{t+2}, ..., o_{T-1} | h_t = s, \theta)
= P(o_{t+1}, ..., o_{T-1} | h_t = s, o_t, \theta) * P(o_t | h_t = s, \theta)
= P(o_{t+1}, ..., o_{T-1} | h_t = s, o_t, \theta) * emit_p(s, o_t)
= \sum_{h_{t+1}=j} P(h_{t+1}=j, o_{t+1}, ..., o_{T-1} | \theta) * emit_p(s, o_t)
= \sum_{h_{t+1}} P(o_{t+1}, ..., o_{T-1} | h_{t+1}=j, h_t = s, o_t, \theta)
    * P(h_{t+1} = j | h_t = s, o_t, \theta)
    * emit_p(s, o_t)
= \sum_{h_{t+1}=j} beta_t(j) * trans_p(s, j) * emit_p(s, o_t)

==> beta_{t-1}(s) = \sum_{h_{t+1}=j} beta_t(j) * trans_p(s, j) * emit_p(s, o_t)
==> beta_t(s) = \sum_{h_{t+2}=j} beta_{t+1}(j) * trans_p(s, j) * emit_p(s, o_{t+1})


gamma_t(s) = P(h_t = s | O, \theta)
= P(O, h_t = s | \theta) / P(O | \theta)

P(O, h_t = s | \theta) = P(o_0, o_1, ..., o_{T-1}, h_t = s | \theta)
= P(o_0, ..., o_t, h_t = s;  o_{t+1}, ..., o_{T-1} |  \theta)
= P(o_0, ..., o_t, h_t = s | \theta)
        * P(o_{t+1}, ..., o_{T-1} | o_0, ..., o_t, h_t = s; \theta)
= alpha_t(s) * P(o_{t+1}, ..., o_{T-1} | h_t = s; \theta)  # assumption of HMM
= alpha_t(s) * beta_t(s)

P(O | \theta) = \sum_{s=0}^{S-1} alpha_t(s) * beta_t(s)

gamma_t(s) = alpha_t(s) * beta_t(s) /
    { \sum_{s'} alpha_t(s') * beta_t(s') }


Q_pi = \sum_{s=0}^{S-1} log pi_s * gamma_0(s)
s.t. \sum_s pi_s = 1

Use lagrange multiplier method,
Lag(Q_pi, lambda) = Q_pi + lambda * (1 - \sum_s pi_s)
= \sum_{h_0=0}^{h_0=S-1} log pi_s * gamma_0(s) + lambda * (1 - \sum_s pi_s)
s.t. lambda >= 0

==>
lambda = \sum_s gamma_0(s) = \sum_s P(h_0 = s | O, \theta) = 1
pi_s = gamma_0(s) / lambda = gamma_0(s)

# === trans_p
Q(\theta, \theta^{old}) = \sum_H {
     log pi_{h_0} * P(H | O, \theta^{old})                               # Only affects pi
    + \sum_{t=0}^{T-2} log trans_p(h_t, h_{t+1}) * P(H | O, \theta^{old})# Only affects trans_p
    + \sum_{t=0}^{T-1} log emit_p(h_t, o_t) * P(H | O, \theta^{old})     # Only affects emit_p
}

Q_trans_p = \sum_H \sum_{t=0}^{T-2} {log trans_p(h_t, h_{t+1}) } * P(H | O, \theta^{old})
= \sum_{i=0}^{S-1}
  \sum_{j=0}^{S-1}
  \sum_{t=0}^{T-2} {log trans_p(h_t=i, h_{t+1}=j)} * P(h_t=i, h_{t+1}=j | O, \theta^{old})
  # HMM assumption,
    trans_p(h_t, h_{t+1}) = P(h_{t+1} | h_t)
    is independent of h_0, h_{t-1}, h_{t+1}, ..., h{T-1}
  # hence \sum_H = \sum_{h_0, h_1, ..., h_{T-1}} = \sum_{h_t, h_{t+1}},
    P(H | O, \theta) = P(h_t, h_{t+1} | O, \theta)

Let P(h_t=i, h_{t+1}=j | O, \theta) = \xi_t(i, j)

Pre-compute for query (Dynamic Programming)
\xi_t(i, j) = P(h_t=i, h_{t+1}=j | O, \theta)
= P(h_t=i, h_{t+1}=j, O | \theta) / P(O | \theta)

P(h_t=i, h_{t+1}=j, O | \theta)
    = alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j)

\xi_t(i, j) = alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j) /
{ \sum_{h_t=i}\sum_{h_{t+1}=j} alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j) }


Q_trans_p =
    \sum_{i=0}^{S-1}
    \sum_{j=0}^{S-1}
    \sum_{t=0}^{T-2} {log trans_p(h_t=i, h_{t+1}=j)} * P(h_t=i, h_{t+1}=j | O, \theta^{old})
= \sum_{h_t=i}\sum_{h_{t+1}=j} {log trans_p(h_t, h_{t+1})} * \xi_t{i, j}

s.t. \sum_s trans_p(i, s) = 1
trans_p(i, s) = \sum_{t=0}^{T-2} \xi_t{i, s} / \sum_{t=0}^{T-2} \gamma_t{i}
NOTE: from t=0 to T-2, total time steps is T-1.


# === emit_p
Q(\theta, \theta^{old}) = \sum_H {
     log pi_{h_0} * P(H | O, \theta^{old})                               # Only affects pi
    + \sum_{t=0}^{T-2} log trans_p(h_t, h_{t+1}) * P(H | O, \theta^{old})# Only affects trans_p
    + \sum_{t=0}^{T-1} log emit_p(h_t, o_t) * P(H | O, \theta^{old})     # Only affects emit_p
}

# HMM assumption, emit_p(h_t, o_t) = P(o_t | h_t),
    only depends on h_t, just need to enumerate on h_t
Q_emit_p = \sum_{s=0}^{S-1} \sum_{t=0}^{T-1} {log emit_p(h_t, o_t)} * P(h_t=s | O, \theta^{old})
= \sum_s \sum_{t=0}^{T-1} {log emit_p(h_t, o_t)} * \gamma_t(s)

s.t. \sum_{v=0}^{V-1} emit_{h_t, o_t=v} = 1
emit_p(s, v) = \sum_{t=0, o_t=v}^{t=T-1} \gamma_t(s) / \sum_{t=0}^{T-1} \gamma_t(s)




(1) init_p,
pi_s = gamma_0(s),
gamma_t(s) = alpha_t(s) * beta_t(s) /
{ \sum_{s'} alpha_t(s') * beta_t(s') }

alpha_0(s) = P(o_0, h_0=s | \theta) = pi_s * emit_p(s, o_0)
alpha_t(s) = { \sum_{i=0}^{S-1} alpha_{t-1}(i) * trans(i, s) }   * emit_p(s, o_t)

beta_{T-1}(s) = 1,
beta_t(s) = trans_p(s, j) * emit_p(s, o_{t+1}) * \sum_{h_{t+2}=j} beta_{t+1}(j)

(2) trans_p(i, s) = \sum_{t=0}^{T-2} \xi_t{i, s} / \sum_{t=0}^{T-2} \gamma_t{i}
\xi_t(i, j) = alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j) /
{ \sum_{h_t=i}\sum_{h_{t+1}=j} alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j) }

(3) emit_p(s, v) = \sum_{t=0, o_t=v}^{t=T-1} \gamma_t(s) / \sum_{t=0}^{T-1} \gamma_t(s)
