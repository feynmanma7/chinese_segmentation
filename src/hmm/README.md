<script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1>Hidden Markov Models</h1>

HMM is directed probabilistic graphical model.


# Components

> $O$: Oberserved sequence, ${o_1, o_2, ..., o_\tau}, o_i=v_k$.

> $H$: Hidden states, ${h_0, h_1, ..., h_\tau}, h_i=s_k$.

# Parameters

> $\pi$: Initial probability of each  hidden state.$(S)$, $S$ is the number of states.

> $T$: Transmit probability between hidden states.$(S, S)$.

> $E$: Emition probability from hidden state to output item. $(S, V)$, $V$ is the number of output vocabs.

# Problems

## 1. Probability of output sequence

> Given the model $\theta=(\pi, T, E)$, compute the probability of an output seqence $O$, $P(O | H, \theta)$.

## 2. Parameter estimation  

> Given training data of $<I, O>$, estimate the parameters $\theta=(\pi, T, E)$

## 3. Decode the hidden states

> Given the model $\theta=(\pi, T, E)$, and the output sequence $O$, get the hidden states $H$ with maximum probability, $H = argmax_{H} P(O | H, \theta)$

# Assumptions

## Markov Assumption

> $P(h_{t+1}|h_t, ..., h_1;o_t; ..., o_1; \theta) = P(h_{t+1}|h_t;\theta)$

## Time independency

> $P(h_{t+1}=j|h_t=i;\theta)=P(h_{k+1}=j|h_k=i;\theta)$

## Output independency

> $P(o_t|h_t, ..., h_1;o_{t-1}, ..., o_1;\theta)=p(o_t|h_t;\theta)$

# Problem 1: (Evaluation) Probability of output sequence

## Brute force

For an output sequence $O$, compute $P(O | \theta)$.

According to the generative principle of this probalistic graphical model, to compute $P(O|\theta)$, $H$ must be specified ($O$ is generated by $H$).

> $P(O|\theta)=\sum_{H}P(O, H|\theta)$
> $=\sum_{I}P(O|H;\theta)P(H|\theta)$

For fixed hidden states $(h_1, h_2, ..., h_\tau)$,
> $P(H|\theta)=\pi_{h_1}T(h_1,h_2)T(h_2, h_3)...T(h_{\tau-1}, h_\tau)$

For the output seqence $o_1, o_2, ..., o_T$,
> $P(O | H; \theta)=E(h_1, o_1), E(h_2, o_2)...E(h_\tau, o_\tau)$


Thus, 

> $P(O|\theta)=\sum_{h_1,h_2,...,h_\tau}P(O|H;\theta)P(H|\theta)$

> $=\sum_{h_1, h_2, ..., h_\tau}\pi_{h_1}E(h_1,o_1)T(h_1, h_2)E(h_2, o_2)...T(h_{\tau-1}, h_\tau)E(h_\tau, o_\tau)$


The time complexity is $O(\tau S^{\tau})$, $S^{\tau}$ for enumurate over the $\tau$ hidden states, $\tau$ for compute $P(O|I;\theta)$ with a fixed hidden state sequence.

## Forward Algorithm

> $P(o_1, o_2, ..., o_t | \theta) = \sum_{s=1}^S P(o_1, o_2, ..., o_t, h_t=s|\theta)$

Denote  
> $\alpha(t, s) = P(o_1, o_2, ..., o_t, h_t=s|\theta)$

### Initializtion

> $\alpha(1, s) = P(o_1, h_1=s|\theta)=\pi_sE(s, o_1), s=1, 2, ..., S$

### Iteration

> $\alpha(t+1, s) = P(o_1, o_2, ..., o_{t+1}, h_{t+1}=s|\theta)$

> $=\sum_{i=1}^S P(o_1, o_2, ..., o_t, h_t=i,o_{t+1}, h_{t+1}=s|\theta)$

> $=\sum_i P(o_1, o_2, ..., o_t|h_t=i, o_{t+1}, h_{t+1}=s;\theta)$

> $* P(o_{t+1}, h_t=i, h_{t+1}=s|\theta)$

> $=\sum_i P(o_1, o_2, ..., o_t|h_t=i;\theta)$ (Output independency assumption)

> $*P(o_{t+1}|h_{t+1}=s, h_t=i;\theta)$

> $* P(h_{t+1}=s, h_t=i|\theta)$

> $=\sum_i P(o_1, o_2, ..., o_t|h_t=i;\theta)$

> $*P(o_{t+1}|h_{t+1}=s;\theta)$

> $*P(h_{t+1}=s|h_t=i;\theta)*P(h_t=i|\theta)$

> $=\sum_i \{P(o_1, o_2, ..., o_t|h_t=i;\theta) * P(h_t=i|\theta)\} * E(s, o_{t+1}) * T(i, s)$

> $=\sum_i P(o_1, o_2, ..., o_t, h_t=i|\theta) * T(i, s) * E(s, o_{t+1})$

Thus, 
> $\alpha(t+1, s)=\sum_{i=1}^S \{\alpha(t, i) T(i, s) \} E(s, o_{t+1}), t=1, 2, ..., T-1$


### Termination
> $P(O|\theta)=\sum_{s=1}^S \alpha(T, s)$

## Backward Algorithm

Denote

> $\beta(t, j) = P(o_{t+1}, ..., o_T|h_t=j;\theta)$

The reason why $\beta$ is defined like this is clarified in the part of `Parameter estimation`.`definition of `$\gamma$.

### Initalization
> $\beta(\tau, j) = 1, j=1,2, ..., S$

### Iteration
> $\beta(t, i) = P(o_{t+1}, ..., o_\tau|h_t=i;\theta)$

> $=\sum_{h_{t+1}=j}P(o_{t+1}, ..., o_\tau, h_{t+1}=j|h_t=i;\theta)$

> $=\sum_j P(o_{t+1}, ..., o_\tau|h_{t+1}=j,h_t=i;\theta)*$

> $P(h_{t+1}=j|h_t=i;\theta)$

> $=\sum_j P(o_{t+1}, ..., o_\tau|h_{t+1}=j;\theta) *$

> $T(i, j)$

> $=\sum_j P(o_{t+2}, ..., o_\tau|h_{t+1}=j, o_{t+1};\theta) *$

> $P(o_{t+1}|h_{t+1}=j;\theta)$

> $T(i, j)$

> $=\sum_j P(o_{t+2}, ..., o_\tau|h_{t+1}=j;\theta) *$

> $E(j, o_{t+1})$

> $T(i, j)$

Thus, 

> $\beta(t, i) = \sum_j\beta(t+1, j) E(j, o_{t+1})T(i, j), t=\tau-1, \tau-2, ..., 1$

### Termination

> $P(O|\theta) = \sum_{h_1=i}\pi_{h_1}E(h_1, o_1)\beta(1, i)$


# Problem 2: (Learning) Parameter estimation 

With hidden random variables, using EM algorithm (Baum-Welch) for parameters estimation.

Complete data is $(O, H)$, $Q$ function is,

> $Q(\theta, \theta^{old}) = \mathbb{E}_H[\log P(O, H|\theta) | O, \theta^{old}]$
> $=\sum_{H}\log P(O, H|\theta) P(H|O, \theta^{old})$


> $\theta = \arg\max_{\theta} Q(\theta, \theta^{old})$

For 
> $P(O, H|\theta) = \pi_{h_1}E(h_1, o_1)T(h_1, h_2)...T(h_{T-1}, h_T)E(h_T, o_T)$

Thus, 
> $Q(\theta, \theta^{old}) = \sum_{H} \log\pi_{h_1} P(H|O, \theta^{old})$ (1)

> $+\sum_{H} \sum_{t=1}^{\tau-1} \log T(h_t, h_{t+1})  P(H|O, \theta^{old})$ (2)

> $+\sum_H \sum_{t=1}^\tau \log E(h_t, o_t)  P(H|O, \theta^{old})$ (3)

Formula (1),(2) and (3) are only depends on $\pi$, $T$ and $E$ respectively. The three types of parameter can be optimized independently.


## $E$

> $Q_E = \sum_H \sum_{t=1}^\tau \log E(h_t, o_t)  P(H|O, \theta^{old})$

Only depends on $h_t$, 

> $Q_E = \sum_{h_t = i} \sum_{t=1}^\tau \log E(h_t, o_t) P(h_t=i|O, \theta^{old})$


### Definition of $\gamma$

To solve for $E$, $P(h_t=i|O, \theta)$ must be solved.

> $\gamma(t, i) = P(h_t=i|O, \theta)$
> $=\frac{P(O, h_t=i|\theta)}{P(O|\theta)}$

For
> $P(O, h_t=i|\theta)$

> $=P(o_1, ..., o_t, h_t=i, o_{t+1}, ..., o_\tau|\theta)$

> $=\sum_{h_{t+1}=j} P(o_1, ..., o_t, h_t=i, h_{t+1}=j, o_{t+1}, ..., o_\tau|\theta)$

> $=\sum_{j} P(o_1, ..., o_t | h_t=i, h_{t+1}=j, o_{t+1}, ..., o_\tau;\theta)*$

> $P(o_{t+1}, ..., o_\tau, h_t=i, h_{t+1}=j|\theta)$

> $=\sum_j P(o_1, ..., o_t|h_t=i; \theta)*$  (output independency)

> $P(o_{t+1}, ..., o_\tau|h_t=i, h_{t+1}=j;\theta)*$

> $P(h_{t+1}=j, h_t=i|\theta)$

> $=\sum_j P(o_1, ..., o_t|h_t=i;\theta)*$

> $P(o_{t+1}, ..., o_\tau|h_{t+1}=j; \theta)*$  (output independency) 

> $P(h_{t+1}=j|h_t=i;\theta) P(h_t=i|\theta)$

> $=\sum_j P(o_1, ..., o_t|h_t=i;\theta) P(h_t=i|\theta) *$ (**The reason $\alpha$ is defined!**)

> $P(o_{t+1}, ..., o_\tau|h_t=i;\theta)$ (**The reason $\beta$ is defined!**)

> $=\sum_j \alpha(t, i) *$ 

> $P(o_{t+1}, ..., o_\tau|h_t=i;\theta)$ 

> $=\sum_j \alpha(t, i) * \beta(t, i)$ (normalization)

Thus,

> $P(O, h_t=i|\theta)=\alpha(t, i) * \beta(t, i)$

Hence, 

> $\gamma(t, i) = P(h_t=i|O, \theta)$
> $=\frac{P(O, h_t=i|\theta)}{P(O|\theta)}$
> $=\frac{\alpha(t, i) * \beta(t, i)}{\sum_{h_t=i}\alpha(t, i)*\beta(t, i)}$


## $T$

> $Q_{T} = \sum_{H} \sum_{t=1}^{\tau-1} \log T(h_t, h_{t+1})  P(H|O, \theta^{old})$

Only depends on $h_t$ and $h_{t+1}$, 

> $Q_T = \sum_{h_t=i} \sum_{h_{t+1}=j} \sum_{t=1}^{\tau-1} \log T(h_t, h_{t+1}) P(h_t=i, h_{t+1}=j|O, \theta^{old})$ (**The reason $\xi$ is defined!**)

### Definition of $\xi$

Denote 

> $\xi(t, i, j)=P(h_t=i, h_{t+1}=j|O, \theta)$

> $=\frac{P(O, h_t=i, h_{t+1}=j|\theta)}{P(O|\theta)}$

For 

> $P(O, h_t=i, h_{t+1}=j|\theta)=P(o_1, ..., o_t, h_t=i, h_{t+1}=j, o_{t+1}, ..., o_\tau|\theta)$

> $=\alpha(t, i)T(i, j)E(j, o_{t+1})\beta(t+1, j), t=1, 2, ..., \tau-1$

Thus, 

> $\xi(t, i, j) = \frac{\alpha(t, i)T(i, j)E(j, o_{t+1})\beta(t+1, j)}{\sum_{h_t=i}\sum_{h_{t+1}=j} \alpha(t, i)T(i, j)E(j, o_{t+1})\beta(t+1, j)}$


## $Pi$
> $Q_{\pi} =  \sum_{H} \log\pi_{h_1} P(H|O, \theta^{old})$

> $s.t. \sum_{s=1}^S \pi_s=1$

> $P(H|O, \theta)=\frac{P(O, H|\theta)}{P(O|\theta)} \propto P(O, H|\theta)$

Note $\pi$ only depends on $h_1$, 

> $Q_{\pi} = \sum_{h_1=i}\log \pi_{h_1} P(h_1=i|O; \theta^{old})$

> $\sum_{h_1=i}\log \pi_{h_1}
\frac{P(O, h_1=i|\theta^{old})}{P(O|
theta^{old})}$

> $=\sum_{h_1} \log \pi_{h_1}
\frac{\alpha(1, h_1)}{P(O|\theta^{old})}$

# Problem 2: Statistic Way

## $E$

> $E(h_t=i, o_t=v) = \frac{count(h_t=i, o_t=v)}{count(h_t=i)}$

## $T$

> $T(h_t=i, h_{t+1}=j) = \frac{count(h_t=i, h_{t+1}=j)}
{count(h_t=i)}$

## $\pi$

> $\pi(s) = \frac{count(h=s)}{count(h)}$


# Problem 3: (Decoding) Decode the hidden states 

To solve, 

> $\hat H = \arg\max_H P(O |H, \theta)$

Let

> $\phi(t, i) = \max_{h_1, h_2, ..., h_{t-1}} P(h_t=i, h_{t-1}, ..., h_1, o_t, ..., o_1|\theta)$

Obviously, 

> $h_T^* = \arg\max_{i} \phi(T, i) E(i, o_T)$

> $\phi(t+1, j) = max_{h_1, ..., h_t}P(h_{t+1}=j, h_t, ..., h_1, o_{t+1}, o_t, ..., o_1|\theta)$

> $=\max_{h_t=i}\max_{h_1, ..., h_{t-1}}P(h_t=i, h_{t-1}, ..., h_1, o_t, ..., o_1|\theta)T(i, j)E(j, o_{t+1})$

> $=\max_i\phi(t, i)T(i, j)E(j, o_{t+1}), t=1, 2, ..., \tau-1$

> $\phi(1, j) = P(h_1=j, o_1|\theta)$

> $=P(o_1|h_1=j;\theta)P(h_1=j|\theta)$

> $=E(j, o_1) \pi(j)$


## To Decode 

> $h_T^* = \arg\max_{i} \phi(T, i) E(i, o_T)$

### Iteration

> $h_t =\arg\max_{i} \phi(t, i) E(i, o_t) * T(i, h_{t+1}^*), t=\tau-1, ..., 1$



To estimate the parameters of HMM: [init_p, trans_p, emit_p]


States = ['B', 'M', 'E', 'S']
B: Begin of a word.
M: Middle of a word.
E: End of a word.
S: Single characteristic word.


Observed values: V = [vocab_0, vocab_1, ..., vocab_{N-1}], N = n_vocab
Observed variables: O, Output sequences, o_t = v[o_t], t=0,1,..., T-1
Hidden variables: H, The hidden sequences, which are the flag of segmentation,
    h_t, t=0,1, ..., T-1
Complete data: (O, H),
O = o_0, o_1, ..., o_{T-1}
H = h_0, h_1, ..., h_{T-1}

HMM assumptions:
(1) Markov assumption, limited history, one-order Markov Chain,
    P(h_t | h_{t-1}, o_{t-1}, h_{t_2}, o_{t-2}..., h_0, o_0) = P(h_t | h_{t-1})
(1) state is independent of time :  P(h_{t+1}=j | h_t=i) = P(h_{t'+1}=j | h_t'=i);
(2) output only depends on current hidden state:
    P(o_t | h_{T-1}, o_{T-1}, ..., h_0, o_0} = P(o_t | h_t)


Likelihood function of the Data:
P(D | \theta) = P(O | \theta) = \sum_h P(O, H | \theta)
    = \sum_h P(O | H; theta) * P(H | \theta)

Using Expectation-Maximization method,
the Q-function is,
Q(\theta, \theta^{old}) = E_H [ log P(O, H | \theta) | O, \theta^{old}]
= \sum_h log P(O, H | \theta) * P(H | O, \theta^{old})

P(O, H | \theta) =
    pi_{h_0} * emit_p(h_0, o_0)
    * trans_p(h_0, h_1) * emit_p(h_1, o_1)
    * trans_p(h_1, h_2) * emit_p(h_2, o_2)
    * ...
    * trans_p(h_{T-2}, h_{T-1}) * emit_p(h_{T-1}, o_{T-1})

Thus,
Q(\theta, \theta^{old}) = \sum_h {
     log pi_{h_0} * P(H | O, \theta^{old})                               # Only affects pi
    + \sum_{i=0}^{T-2} log trans_p(h_i, h_{i+1}) * P(H | O, \theta^{old})# Only affects trans_p
    + \sum_{i=0}^{T-1} log emit_p(h_i, o_i) * P(H | O, \theta^{old})     # Only affects emit_p
}

# === pi
Considering components Q_pi only affects pi in Q, s.t. \sum_s pi_s = 1
Q_pi = \sum_h log pi_{h_0} * P(H | \theta^{old})
= \sum_{s=0}^{S-1} log pi_s * P(h_0 = s | O, \theta^{old} )

Let gamma_t(s) = P(h_t = s | O, \theta)
pi_s = gamma_0(s)

Currently, I don't know why the forward and backward prob is defined, as below.

Forward probability, given hmm model(\theta), in time t, the hidden state is s,
the observed sequence from time 0 to time t is o_0, o_1, ..., o_t,
alpha_t(s) = P(o_0, o_1, ..., o_t, h_t = s | \theta)
alpha_0(s) = pi_s * trans_p(s, o_0)
alpha_t(s) = {\sum_{i=0}^{S-1} alpha_{t-1}(i) * trans(i, s)} * emit_p(s, o_t)
P(O | \theta) = \sum_{s=0}^{S-1} alpha_{T-1}(s)


Backward probability, given hmm model, in time t, the hidden state is s,
the observed sequence from time t+1 to time T-1 is o_{t+1}, o_{t+2}, ..., o_{T-1}
beta_t(s) = P(o_{t+1}, o_{t+2}, ..., o_{T-1} | h_t = s, \theta)
beta_{t-1}(s) = P(o_t, o_{t+1}, o_{t+2}, ..., o_{T-1} | h_t = s, \theta)
= P(o_{t+1}, ..., o_{T-1} | h_t = s, o_t, \theta) * P(o_t | h_t = s, \theta)
= P(o_{t+1}, ..., o_{T-1} | h_t = s, o_t, \theta) * emit_p(s, o_t)
= \sum_{h_{t+1}=j} P(h_{t+1}=j, o_{t+1}, ..., o_{T-1} | \theta) * emit_p(s, o_t)
= \sum_{h_{t+1}} P(o_{t+1}, ..., o_{T-1} | h_{t+1}=j, h_t = s, o_t, \theta)
    * P(h_{t+1} = j | h_t = s, o_t, \theta)
    * emit_p(s, o_t)
= \sum_{h_{t+1}=j} beta_t(j) * trans_p(s, j) * emit_p(s, o_t)

==> beta_{t-1}(s) = \sum_{h_{t+1}=j} beta_t(j) * trans_p(s, j) * emit_p(s, o_t)
==> beta_t(s) = \sum_{h_{t+2}=j} beta_{t+1}(j) * trans_p(s, j) * emit_p(s, o_{t+1})


gamma_t(s) = P(h_t = s | O, \theta)
= P(O, h_t = s | \theta) / P(O | \theta)

P(O, h_t = s | \theta) = P(o_0, o_1, ..., o_{T-1}, h_t = s | \theta)
= P(o_0, ..., o_t, h_t = s;  o_{t+1}, ..., o_{T-1} |  \theta)
= P(o_0, ..., o_t, h_t = s | \theta)
        * P(o_{t+1}, ..., o_{T-1} | o_0, ..., o_t, h_t = s; \theta)
= alpha_t(s) * P(o_{t+1}, ..., o_{T-1} | h_t = s; \theta)  # assumption of HMM
= alpha_t(s) * beta_t(s)

P(O | \theta) = \sum_{s=0}^{S-1} alpha_t(s) * beta_t(s)

gamma_t(s) = alpha_t(s) * beta_t(s) /
    { \sum_{s'} alpha_t(s') * beta_t(s') }


Q_pi = \sum_{s=0}^{S-1} log pi_s * gamma_0(s)
s.t. \sum_s pi_s = 1

Use lagrange multiplier method,
Lag(Q_pi, lambda) = Q_pi + lambda * (1 - \sum_s pi_s)
= \sum_{h_0=0}^{h_0=S-1} log pi_s * gamma_0(s) + lambda * (1 - \sum_s pi_s)
s.t. lambda >= 0

==>
lambda = \sum_s gamma_0(s) = \sum_s P(h_0 = s | O, \theta) = 1
pi_s = gamma_0(s) / lambda = gamma_0(s)

# === trans_p
Q(\theta, \theta^{old}) = \sum_H {
     log pi_{h_0} * P(H | O, \theta^{old})                               # Only affects pi
    + \sum_{t=0}^{T-2} log trans_p(h_t, h_{t+1}) * P(H | O, \theta^{old})# Only affects trans_p
    + \sum_{t=0}^{T-1} log emit_p(h_t, o_t) * P(H | O, \theta^{old})     # Only affects emit_p
}

Q_trans_p = \sum_H \sum_{t=0}^{T-2} {log trans_p(h_t, h_{t+1}) } * P(H | O, \theta^{old})
= \sum_{i=0}^{S-1}
  \sum_{j=0}^{S-1}
  \sum_{t=0}^{T-2} {log trans_p(h_t=i, h_{t+1}=j)} * P(h_t=i, h_{t+1}=j | O, \theta^{old})
  # HMM assumption,
    trans_p(h_t, h_{t+1}) = P(h_{t+1} | h_t)
    is independent of h_0, h_{t-1}, h_{t+1}, ..., h{T-1}
  # hence \sum_H = \sum_{h_0, h_1, ..., h_{T-1}} = \sum_{h_t, h_{t+1}},
    P(H | O, \theta) = P(h_t, h_{t+1} | O, \theta)

Let P(h_t=i, h_{t+1}=j | O, \theta) = \xi_t(i, j)

Pre-compute for query (Dynamic Programming)
\xi_t(i, j) = P(h_t=i, h_{t+1}=j | O, \theta)
= P(h_t=i, h_{t+1}=j, O | \theta) / P(O | \theta)

P(h_t=i, h_{t+1}=j, O | \theta)
    = alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j)

\xi_t(i, j) = alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j) /
{ \sum_{h_t=i}\sum_{h_{t+1}=j} alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j) }


Q_trans_p =
    \sum_{i=0}^{S-1}
    \sum_{j=0}^{S-1}
    \sum_{t=0}^{T-2} {log trans_p(h_t=i, h_{t+1}=j)} * P(h_t=i, h_{t+1}=j | O, \theta^{old})
= \sum_{h_t=i}\sum_{h_{t+1}=j} {log trans_p(h_t, h_{t+1})} * \xi_t{i, j}

s.t. \sum_s trans_p(i, s) = 1
trans_p(i, s) = \sum_{t=0}^{T-2} \xi_t{i, s} / \sum_{t=0}^{T-2} \gamma_t{i}
NOTE: from t=0 to T-2, total time steps is T-1.


# === emit_p
Q(\theta, \theta^{old}) = \sum_H {
     log pi_{h_0} * P(H | O, \theta^{old})                               # Only affects pi
    + \sum_{t=0}^{T-2} log trans_p(h_t, h_{t+1}) * P(H | O, \theta^{old})# Only affects trans_p
    + \sum_{t=0}^{T-1} log emit_p(h_t, o_t) * P(H | O, \theta^{old})     # Only affects emit_p
}

# HMM assumption, emit_p(h_t, o_t) = P(o_t | h_t),
    only depends on h_t, just need to enumerate on h_t
Q_emit_p = \sum_{s=0}^{S-1} \sum_{t=0}^{T-1} {log emit_p(h_t, o_t)} * P(h_t=s | O, \theta^{old})
= \sum_s \sum_{t=0}^{T-1} {log emit_p(h_t, o_t)} * \gamma_t(s)

s.t. \sum_{v=0}^{V-1} emit_{h_t, o_t=v} = 1
emit_p(s, v) = \sum_{t=0, o_t=v}^{t=T-1} \gamma_t(s) / \sum_{t=0}^{T-1} \gamma_t(s)




(1) init_p,
pi_s = gamma_0(s),
gamma_t(s) = alpha_t(s) * beta_t(s) /
{ \sum_{s'} alpha_t(s') * beta_t(s') }

alpha_0(s) = P(o_0, h_0=s | \theta) = pi_s * emit_p(s, o_0)
alpha_t(s) = { \sum_{i=0}^{S-1} alpha_{t-1}(i) * trans(i, s) }   * emit_p(s, o_t)

beta_{T-1}(s) = 1,
beta_t(s) = trans_p(s, j) * emit_p(s, o_{t+1}) * \sum_{h_{t+2}=j} beta_{t+1}(j)

(2) trans_p(i, s) = \sum_{t=0}^{T-2} \xi_t{i, s} / \sum_{t=0}^{T-2} \gamma_t{i}
\xi_t(i, j) = alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j) /
{ \sum_{h_t=i}\sum_{h_{t+1}=j} alpha_t(i) * trans_p{i, j} * emit_p_{t+1}(j) * beta_{t+1}(j) }

(3) emit_p(s, v) = \sum_{t=0, o_t=v}^{t=T-1} \gamma_t(s) / \sum_{t=0}^{T-1} \gamma_t(s)
